# kabobs

https://open.kattis.com/problems/kabobs

Kabobs is a problem in which you are given a number of rules and then asked to calculate the number of strings of a given length that fit those rules. The rules consist of a setter and a mask, where if a setter occurs in a string, the mask must occur at some point after it. A given mask may cover multiple occurences of a set, but not any that appear after it. As a nicety, we are given that no letter occurs multiple times in a single rule, so there's no worries about setters/masks occuring at the same location. For bounds we have 500 as the length of the string, 62 as our alphabet size, and 60 as a number of characters describing our rules. Clearly there is far too many strings to generate and count. What about memoization? If we could find some notion of an interesting set of characteristics that a prefix string might have, we could memoize based on that. Naively, we want to know what rules are set and what any interesting prefix that we might have. Let's say any prefix that occurs in our rules is interesting, so there's a max of 60 or so. The memoized time complexity is now (2^15 rules \* 500 lengths \* 60 prefixes \* 62 chars in the alphabet) That's about 60b - far too large. Even if we consider that there's probably some impossible states in there there's no way that's going to work. So how can we further reduce the number of things we need to keep track of? Lets consider a prefix P that only occurs in a single set rule. If this rule has already been set we don't really care about this prefix anymore because setting a rule twice leaves it as set and doesn't change our state. This means that we don't need to pass the prefix through our memo: it looks and behaves just like some smaller state. There might be a number of different states that consolidate down to a single one. Let's agressively follow this line: for each ruleset / prefix pair we will look forward and see what rule parts we might be interesting prefixes of and cache those setters/masks; next we will make each move for each rule set / prefix pair and follow that state through it's similar shorter prefix states until we find some state that has a cached setter or mask that we still care about; Finally, we will store the counts and targets for each move that could be made from a given pair and use those as forward steps in our memoized search. Now to find our time complexity let's think about how many states we might have and how many different interesting moves we might have from each state. For a given state to be interesting it can have any ruleset, but a limited number of prefixes determined by our ruleset. For each rule that is set we can disregard any prefix of that rules setter and for any rule that is not set we can disregard any prefix of the mask. Furthermore, since a full rule string always either sets or masks its rule we can disregard any full-rule string. This cuts the number of prefixes more than in half. For any prefix to be interesting at all it must be part of some rule string with length >= 3 (A>BC for example), which means that we could have 2^15=30k states using 2s, 2^12\*12=50k different states using 3s, or 2^10\*20=20k using 4s and smaller from there. This is much better than the previous 20m different states. Similarly, each move has to lead to a different interesting to be distinct, we have <= 20 different interesting prefixes while we're in our worst case, and we don't care about a given prefix half of the time which means that the number of moves out is a factor of <=10 if we keep them as target/count pairs. Finally we have to do 500 moves on this board which leaves us with an approxmiate order of 50k\*6\*500=150m in 4s, which is just on the edge of doable if we use a table and iterative methods.

An alternate way to approach the problem is to recognize that it can be represented as a DFA, construct the DFA using DFA operators for in sequence and product (matches both), and run the DFA forwards. This has a similar time complexity as used above, but requires first understanding a DFA and how to use it. I programmed kabobs_dfa based on that idea, and then used my new knowledge to try and reconstruct it as a series of optimizations from a well-understood solution which became kabobs_table. Without knowledge of a DFA this problem is difficult but not impossible; the above optimizations require a few significant leaps and end up with you implementing most of a DFA, but could come about from normal exploration of the problem. I've suggested that we add a DFA implementation to our 25 pages, along with some classic algorithms on it such as equivalence, union, intersection, and count.